{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaMittapelly/SaiPriyanka_INFO5731_Spring2020/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "vR0L3_CreM_A",
        "outputId": "d0c72f7a-c172-4072-b4ff-47779cff480e"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "lis = []\n",
        "emp = []\n",
        "df = open('AdamsTanner.txt', \"r\")\n",
        "lines = df.readlines()\n",
        "for i in lines :\n",
        "    lis.append(i)\n",
        "df.close()\n",
        "#df1 = pd.DataFrame(zip(lis , emp ) ,columns=[\"DATA\" , \"word_count\"])\n",
        "df1 = pd.DataFrame(lis , columns=[\"DATA\"])\n",
        "\n",
        "\n",
        "df1['DATA'] = df1['DATA'].str.replace(\"\\n\",\" \")\n",
        "df1\n",
        "\n",
        "df1.to_csv('out.csv', index=False)\n",
        "text = open(\"out.csv\", \"r\") \n",
        "'''text = ''.join([i for i in text])  \n",
        "text = text.replace(\"/n\", \" \") \n",
        "x = open(\"final.csv\",\"w\") \n",
        "x.writelines(text) \n",
        "x.close() '''  \n",
        "\n",
        "# count senetences\n",
        "\n",
        "data =  \"/content/out.csv\"\n",
        "\n",
        "word = pd.read_csv(data)\n",
        "word['sentence_count'] = word['DATA'].apply(lambda x : len(str(x).split(\".\")))\n",
        "word['word_count'] = word['DATA'].apply(lambda x : len(str(x).split(\" \")))\n",
        "word['char_count'] = word['DATA'].str.len()\n",
        "word['hashtags_rem'] = word['DATA'].apply(lambda x :len([x for x in  x.split() if x.startswith('#') ]))\n",
        "word['digit'] = word['DATA'].apply(lambda x :len([x for x in  x.split() if x.isdigit()]))\n",
        "word['upper'] = word['DATA'].apply(lambda x :len([x for x in  x.split() if x.isupper()]))\n",
        "word[['DATA' ,'sentence_count' , 'word_count' , 'char_count','hashtags_rem' , 'digit' , 'upper']] \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>hashtags_rem</th>\n",
              "      <th>digit</th>\n",
              "      <th>upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>History</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  DATA  ...  upper\n",
              "0                                          5 Ala. 740   ...      0\n",
              "1                           Supreme Court of Alabama.   ...      0\n",
              "2                                               ADAMS   ...      1\n",
              "3                                                  v.   ...      0\n",
              "4                                  TANNER AND HORTON.   ...      3\n",
              "..                                                 ...  ...    ...\n",
              "143           There are no Filings for this citation.   ...      0\n",
              "144                                Negative Treatment   ...      0\n",
              "145  There are no Negative Treatment results for th...  ...      0\n",
              "146                                           History   ...      0\n",
              "147   There are no History results for this citation.   ...      0\n",
              "\n",
              "[148 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "26pEOuVn8NXs",
        "outputId": "77f7abd3-0607-4619-97af-31472c1ea5f7"
      },
      "source": [
        "import pandas as pd \r\n",
        "\r\n",
        "data =  \"/content/out.csv\"\r\n",
        "\r\n",
        "word = pd.read_csv(data)\r\n",
        "\r\n",
        "def avg_cou(sentence):\r\n",
        "    values = sentence.split()\r\n",
        "    l = len(values)\r\n",
        "    v = sum(len(val) for val in values)\r\n",
        "    try:\r\n",
        "     z = v/l\r\n",
        "     return z\r\n",
        "    except ZeroDivisionError:\r\n",
        "     z = 0\r\n",
        "\r\n",
        "\r\n",
        "word['avg_cou'] = word['DATA'].apply(lambda x : avg_cou(x))\r\n",
        "word[['DATA','avg_cou']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA</th>\n",
              "      <th>avg_cou</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>2.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>5.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>4.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>8.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>5.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>History</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  DATA   avg_cou\n",
              "0                                          5 Ala. 740   2.666667\n",
              "1                           Supreme Court of Alabama.   5.500000\n",
              "2                                               ADAMS   5.000000\n",
              "3                                                  v.   2.000000\n",
              "4                                  TANNER AND HORTON.   5.333333\n",
              "..                                                 ...       ...\n",
              "143           There are no Filings for this citation.   4.714286\n",
              "144                                Negative Treatment   8.500000\n",
              "145  There are no Negative Treatment results for th...  5.555556\n",
              "146                                           History   7.000000\n",
              "147   There are no History results for this citation.   5.000000\n",
              "\n",
              "[148 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6CngMZyNJftp",
        "outputId": "8b823284-9e3a-4c3a-854c-a39e4f5a61ba"
      },
      "source": [
        "from nltk.corpus  import  stopwords\r\n",
        "import pandas as pd \r\n",
        "import nltk\r\n",
        "\r\n",
        "data =  \"/content/out.csv\"\r\n",
        "\r\n",
        "word = pd.read_csv(data)\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "stop = stopwords.words('english')\r\n",
        "word['stop_count'] = word['DATA'].apply(lambda x :len([x for x in  x.split() if x in stop ]))\r\n",
        "\r\n",
        "word[['DATA' ,'stop_count']] \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA</th>\n",
              "      <th>stop_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>History</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  DATA  stop_count\n",
              "0                                          5 Ala. 740            0\n",
              "1                           Supreme Court of Alabama.            1\n",
              "2                                               ADAMS            0\n",
              "3                                                  v.            0\n",
              "4                                  TANNER AND HORTON.            0\n",
              "..                                                 ...         ...\n",
              "143           There are no Filings for this citation.            4\n",
              "144                                Negative Treatment            0\n",
              "145  There are no Negative Treatment results for th...           4\n",
              "146                                           History            0\n",
              "147   There are no History results for this citation.            4\n",
              "\n",
              "[148 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybyzQe4Lzcm",
        "outputId": "583e6c0f-41e3-4790-ad35-4596991b251e"
      },
      "source": [
        "#QUESTION 1.2 \r\n",
        "import nltk\r\n",
        "import pandas as pd \r\n",
        "from nltk.corpus  import  stopwords\r\n",
        "\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "import string\r\n",
        "\r\n",
        "from textblob import TextBlob , Word \r\n",
        "\r\n",
        "data =  \"/content/out.csv\"\r\n",
        "\r\n",
        "word = pd.read_csv(data)\r\n",
        "#ALL CHAR TO LOWER CASE\r\n",
        "word['DATA'] = word['DATA'].apply(lambda x :\" \".join(x.lower() for x in x.split()))\r\n",
        "word['DATA']\r\n",
        "type1 = pd.DataFrame(word)\r\n",
        "\r\n",
        "\r\n",
        "# STOP WORDS REMOVAL\r\n",
        "nltk.download('stopwords')\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "type1['DATA'] = type1['DATA'].apply(lambda x :\" \".join([x for x in  x.split() if x not in stop ]))\r\n",
        "type2 = pd.DataFrame(type1)\r\n",
        "\r\n",
        "#punctuation\r\n",
        "\r\n",
        "type2['DATA'] = type2['DATA'].apply(lambda x:''.join([i for i in x \r\n",
        "                                                  if i not in string.punctuation]))\r\n",
        "type3 = pd.DataFrame(type2)\r\n",
        "\r\n",
        "#FREQUENT WORDS REMOVAL\r\n",
        "\r\n",
        "fre = pd.Series(' '.join(type2['DATA']).split()).value_counts()[:20]\r\n",
        "fre = list(fre.index)\r\n",
        "type3['DATA'] = type3['DATA'].apply(lambda x :\" \".join([x for x in  x.split() if x not in fre ]))\r\n",
        "type4 = pd.DataFrame(type3)\r\n",
        "\r\n",
        "# RARE WORDS REMOVAL\r\n",
        "\r\n",
        "rar = pd.Series(' '.join(type2['DATA']).split()).value_counts()[-20:]\r\n",
        "rar = list(rar.index)\r\n",
        "type4['DATA'] = type4['DATA'].apply(lambda x :\" \".join([x for x in  x.split() if x not in rar ]))\r\n",
        "type5 = pd.DataFrame(type4)\r\n",
        "type5\r\n",
        "\r\n",
        "# SPELLING CORRECTION \r\n",
        "\r\n",
        "\r\n",
        "type5['DATA'] = type5['DATA'] .apply(lambda x : str(TextBlob(x).correct()))\r\n",
        "type6 = pd.DataFrame(type5)\r\n",
        "\r\n",
        "#toeknization\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# STEMMING \r\n",
        "st = PorterStemmer()\r\n",
        "type6['DATA'] = type5['DATA'].apply(lambda x : \" \".join([st.stem(info) for info in x.split()]))\r\n",
        "type7 = pd.DataFrame(type6)\r\n",
        "\r\n",
        "\r\n",
        "#lemmatization\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "type7['DATAafter cleaning'] = type6['DATA'].apply(lambda x : \" \".join([Word(values).lemmatize() for values in x.split()]))\r\n",
        "df1 = pd.DataFrame(type7)\r\n",
        "type8\r\n",
        "\r\n",
        "# all data after cleaning , storing in new datacleaning colum \r\n",
        "\r\n",
        "df1.to_csv('datacleaning.csv', index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I37xaUZXcV0l",
        "outputId": "2d074556-6587-4f2d-a48c-5a9ef84d331d"
      },
      "source": [
        "import nltk\r\n",
        "import pandas as pd \r\n",
        "from textblob import TextBlob\r\n",
        "# TOKENIZATION \r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "TextBlob(type4['DATA'][2]).words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['adam'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "UNR0ppX5XTBm",
        "outputId": "7862f67f-34ce-4955-9ec3-93bba7076d84"
      },
      "source": [
        "#Calculate the term frequency of all the terms.\r\n",
        "tf1 = (word['DATA'].apply(lambda x : pd.value_counts(x.split(\" \" ))).sum(axis=0).reset_index())\r\n",
        "tf1.columns = ['word' ,'tf']\r\n",
        "tf1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>740</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>suprem</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alabama</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>adam</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>half</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>let</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>file</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>neg</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>histori</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>734 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        word   tf\n",
              "0        740  2.0\n",
              "1          5  8.0\n",
              "2     suprem  1.0\n",
              "3    alabama  1.0\n",
              "4       adam  1.0\n",
              "..       ...  ...\n",
              "729     half  1.0\n",
              "730      let  1.0\n",
              "731     file  2.0\n",
              "732      neg  2.0\n",
              "733  histori  2.0\n",
              "\n",
              "[734 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP0epwFbwfVj",
        "outputId": "6797f299-451c-4ee0-9028-a79151eb0c76"
      },
      "source": [
        "from textblob  import Word\r\n",
        "import nltk\r\n",
        "import pandas as pd \r\n",
        "nltk.download('wordnet')\r\n",
        "word['DATA'] = word['DATA'].apply(lambda x : \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "\r\n",
        "print(\"N GRAM 2\")\r\n",
        "\r\n",
        "for i in range(11):\r\n",
        "  print(TextBlob(word['DATA'][i]).ngrams(2))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[WordList(['5', '740'])]\n",
            "[WordList(['suprem', 'alabama'])]\n",
            "[]\n",
            "[]\n",
            "[WordList(['manner', 'norton'])]\n",
            "[WordList(['june', 'term']), WordList(['term', '1843'])]\n",
            "[]\n",
            "[WordList(['writ', 'error']), WordList(['error', 'circuit']), WordList(['circuit', 'sumter'])]\n",
            "[]\n",
            "[WordList(['west', 'headnot'])]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53jOQVwB1e7V",
        "outputId": "679564eb-a586-4bb4-f8d2-48aeee83c285"
      },
      "source": [
        "from textblob  import Word\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "word['DATA'] = word['DATA'].apply(lambda x : \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "print( \" N 3 GRAM \")\r\n",
        "\r\n",
        "for i in range(11):\r\n",
        "  print(TextBlob(word['DATA'][i]).ngrams(3))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            " N 3 GRAM \n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[WordList(['june', 'term', '1843'])]\n",
            "[]\n",
            "[WordList(['writ', 'error', 'circuit']), WordList(['error', 'circuit', 'sumter'])]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCKvw6qF1Uih"
      },
      "source": [
        "from textblob  import Word\r\n",
        "import pandas as pd \r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "word['DATA'] = word['DATA'].apply(lambda x : \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "print( \" N 1 GRAM \")\r\n",
        "\r\n",
        "for i in range(11):\r\n",
        "  print(TextBlob(word['DATA'][i]).ngrams(1))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv6fVhOfFmv",
        "outputId": "6a3455c8-b6ad-49cc-d4bb-8ce6e505bf2f"
      },
      "source": [
        "\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "new_ip = \".\".join([str(int(i)) for i in ip.split(\".\")])   \n",
        "print(new_ip)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdJpDx9gjbX",
        "outputId": "fd69b08a-2ad6-4ac1-b66d-bf2d73d97066"
      },
      "source": [
        "import re \r\n",
        "sentence =  \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "res = re.findall('2\\d\\d\\d', sentence)\r\n",
        "print(res)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}