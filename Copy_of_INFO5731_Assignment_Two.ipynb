{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaMittapelly/SaiPriyanka_INFO5731_Spring2020/blob/main/Copy_of_INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup \n",
        "import bs4\n",
        "from urllib.request import urlopen as fopen  \n",
        "import requests\n",
        "import pandas as pd \n",
        "import csv\n",
        "\n",
        "url = \"https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc\"\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text , 'html.parser')\n",
        "\n",
        "table = soup.find_all('div' , {'class': 'pubabstract'})\n",
        "data = table \n",
        "output = []\n",
        "df = pd.DataFrame()\n",
        "for temp in data:\n",
        "    output = temp.text\n",
        "    #print(output)\n",
        "    df = df.append(pd.Series(output),ignore_index=True).replace('\\n' , ' ')\n",
        "output1 = []\n",
        "for a in range(10 , 100, 10):\n",
        "    url_next ='https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start={}' .format(a) \n",
        "    urls = url_next\n",
        "    page1 = requests.get(url_next)\n",
        "    #print(urls)\n",
        "    soup1 = BeautifulSoup(page1.text , 'html.parser')\n",
        "    table1 = soup1.find_all('div' , {'class': 'pubabstract'}) \n",
        "    data1 = table1\n",
        "    #print(data1)\n",
        "    #print('@@@@@@@@@@@@@@@@@@@@@@')\n",
        "    for temp1 in data1:\n",
        "        output1.append(temp1.text)\n",
        "df1 = pd.DataFrame(output1)\n",
        "df2 = pd.concat([df,df1])\n",
        "Finaldf = pd.DataFrame(df2)\n",
        "\n",
        "Finaldf.columns=['Abstract']\n",
        "Finaldf['Abstract'] = Finaldf['Abstract'].str.replace(\"\\n\",\" \")\n",
        "\n",
        "#CONVERTING TO CSV FILE \n",
        "\n",
        "Finaldf.to_csv('Abstract1.csv', index=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbHCga7BNC8D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "098a2b9f-ef60-4979-9e7f-4e63bd0ff051"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "\n",
        "\n",
        "from nltk.corpus  import  stopwords\n",
        "import pandas as pd \n",
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import string\n",
        "\n",
        "from textblob import TextBlob , Word\n",
        "\n",
        "\n",
        "csvfile = '/content/Abstract1.csv'\n",
        "Abstractlis = pd.read_csv(csvfile)\n",
        "#print(Abstractlis)\n",
        "spec = [\" ! , @ , #, $ , ^ , & , * , '  '  \"]\n",
        "\n",
        "#******specialchar*********\n",
        "\n",
        "Abstractlis['specialchar_rem'] = Abstractlis['Abstract'].str.replace( [\" !,@,#,$,%,^,&,*,(),[],{},;,:,.,<>,?,~,-,=,+,|,/,\\ \" ] , \" \")\n",
        "\n",
        "#*******8REMOVENUMBERS*********\n",
        "\n",
        "Abstractlis['numbers'] = Abstractlis['Abstract'].str.replace('\\d+', '')\n",
        "\n",
        "#*******STOPWORDS******\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "Abstractlis['stop_count'] = Abstractlis['Abstract'].apply(lambda x :([x for x in  x.split() if x not in stop ]))\n",
        "\n",
        "#********LOWER********\n",
        "\n",
        "Abstractlis['Lower'] = Abstractlis['Abstract'].str.lower()\n",
        "\n",
        "\n",
        "#******STEMMING********\n",
        "\n",
        "st = PorterStemmer()\n",
        "Abstractlis['stemming'] = Abstractlis['Abstract'].apply(lambda x : \" \".join([st.stem(info) for info in x.split()]))\n",
        "\n",
        "\n",
        "#********LEMMATIZE*****\n",
        "\n",
        "nltk.download('wordnet')\n",
        "Abstractlis['lemmatizing'] = Abstractlis['Abstract'].apply(lambda x : \" \".join([Word(values).lemmatize() for values in x.split()]))\n",
        "\n",
        "Abstractlis[['Abstract' ,'special_rem', 'numbers' , 'stop_count' , 'Lower' ,'stemming', 'lemmatizing' ]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>hashtags_rem</th>\n",
              "      <th>numbers</th>\n",
              "      <th>stop_count</th>\n",
              "      <th>Lower</th>\n",
              "      <th>stemming</th>\n",
              "      <th>lemmatizing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>[Abstract, found]</td>\n",
              "      <td>abstract not found       ...</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>Abstract not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>[describe, method, statistical, modeling, base...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describ a method for statist model base on max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>[Scaling, conditional, random, fields, natural...</td>\n",
              "      <td>scaling conditional rando...</td>\n",
              "      <td>scale condit random field for natur languag pr...</td>\n",
              "      <td>Scaling conditional random field for natural l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>[The, paper, addresses, issue, cooperation, li...</td>\n",
              "      <td>the paper addresses the i...</td>\n",
              "      <td>the paper address the issu of cooper between l...</td>\n",
              "      <td>The paper address the issue of cooperation bet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>[In, natural, language, processing, applicatio...</td>\n",
              "      <td>in most natural language ...</td>\n",
              "      <td>In most natur languag process applications, de...</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>[This, paper, presents, workbench, built, Prib...</td>\n",
              "      <td>this paper presents a wor...</td>\n",
              "      <td>thi paper present a workbench built by pribera...</td>\n",
              "      <td>This paper present a workbench built by Priber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>[Abstract—Natural, Language, Processing, (NLP)...</td>\n",
              "      <td>abstract—natural language...</td>\n",
              "      <td>abstract—natur languag process (nlp) is an eff...</td>\n",
              "      <td>Abstract—Natural Language Processing (NLP) is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>[ABSTRACT:, After, twenty, years, disfavor,, t...</td>\n",
              "      <td>abstract: after twenty ye...</td>\n",
              "      <td>abstract: after twenti year of disfavor, a tec...</td>\n",
              "      <td>ABSTRACT: After twenty year of disfavor, a tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>[Text, statistics, frequently, used, stylometr...</td>\n",
              "      <td>text statistics are frequ...</td>\n",
              "      <td>text statist are frequent use in stylometri an...</td>\n",
              "      <td>Text statistic are frequently used in stylomet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>[We, summarize, experience, using, FrameNet, t...</td>\n",
              "      <td>we summarize our experien...</td>\n",
              "      <td>We summar our experi use framenet in two rathe...</td>\n",
              "      <td>We summarize our experience using FrameNet in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstract  ...                                        lemmatizing\n",
              "0                        Abstract not found       ...  ...                                 Abstract not found\n",
              "1                         describe a method for st...  ...  describe a method for statistical modeling bas...\n",
              "2                        Scaling conditional rando...  ...  Scaling conditional random field for natural l...\n",
              "3                        The paper addresses the i...  ...  The paper address the issue of cooperation bet...\n",
              "4                        In most natural language ...  ...  In most natural language processing applicatio...\n",
              "..                                                ...  ...                                                ...\n",
              "95                       This paper presents a wor...  ...  This paper present a workbench built by Priber...\n",
              "96                       Abstract—Natural Language...  ...  Abstract—Natural Language Processing (NLP) is ...\n",
              "97                       ABSTRACT: After twenty ye...  ...  ABSTRACT: After twenty year of disfavor, a tec...\n",
              "98                       Text statistics are frequ...  ...  Text statistic are frequently used in stylomet...\n",
              "99                       We summarize our experien...  ...  We summarize our experience using FrameNet in ...\n",
              "\n",
              "[100 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58249b96-50e0-4a35-e5cc-c78c9237b8a2"
      },
      "source": [
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "posdata = []\n",
        "divide ={}\n",
        "final = []\n",
        "temp5 = []\n",
        "noun = 0 \n",
        "verb = 0\n",
        "adjective = 0\n",
        "adverb = 0\n",
        "with open('/content/Abstract1.csv' , 'r') as input:\n",
        "  reader = csv.reader(input)\n",
        "  for row in reader:\n",
        "    posdata.extend(row)\n",
        "#print(posdata)\n",
        "for y in posdata:\n",
        "  temp3 = y.split()\n",
        "  tokens_tag = pos_tag(temp3,tagset='universal')\n",
        "  divide.update(dict(tokens_tag))\n",
        "#print(divide)\n",
        "#temporary = dict(divide)\n",
        "for k, v  in divide.items():\n",
        "  if v == 'NOUN':\n",
        "    noun = noun+ 1 \n",
        "\n",
        "  if v == 'ADJ':\n",
        "    adjective = adjective + 1 \n",
        "\n",
        "  if v == 'VERB':\n",
        "    verb = verb + 1\n",
        "\n",
        "  if v == 'ADV':\n",
        "    adverb = adverb + 1\n",
        "\n",
        "print(noun)\n",
        "print(adjective)\n",
        "print(verb)\n",
        "print(adverb)\n",
        "         \n",
        "#print(selective_pos_words)\n",
        "#df4 = pd.DataFrame(divide)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#posdata = nltk.pos_tag(divide)\n",
        "#print(posdata)\n",
        "\n",
        "#posdf = pd.DataFrame(posdata)\n",
        "#posdf\n",
        "\n",
        "\n",
        "#text = word_tokenize(posdata)\n",
        "#print(nltk.pos_tag(text))\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "675\n",
            "280\n",
            "291\n",
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "Constituency Parsing :\r\n",
        "\r\n",
        "According to my knowledge , first step is we need to identify pos for all the words in sentence and  parsing has got some rules to be followed for catagorize the words based on the POS . rules may include 1)Sentence should be catagorized with NP VP . The NP and VP may further divide into sub parsing based on the pos . Prsing will end untill it reaches the end of the statement .learning CKY algorithm will help to do parsing \r\n",
        "Noun phrase - determiner Nominal\r\n",
        "Verp Phrase - V NP\r\n",
        "Nominal - Noun | Nominal \r\n",
        "if we want to do consistency parsing for a sentence we need to follow the the rules above mentioned and if we know the rules we can understand how does parsing  was done and its been catagoerized .\r\n",
        "\r\n",
        "The sentence for which Constituency Parsing was done is \r\n",
        " \" This is a new approach in natural language processing based on the deterministic chaotic behavior of dynamical systems. \"\r\n",
        "\r\n",
        " words are first been divided into pos \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        " (NP (NP (NN process)) (PP (IN of) (NP (NN language) (NN understanding))))\r\n",
        "\r\n",
        " Sentence is been divide into NP  and NP \r\n",
        "S = NP and NP \r\n",
        " 1)NP  divided into NN and PP\r\n",
        " 2)NP divided into NN and NN\r\n",
        "\r\n",
        "\r\n",
        "DEPENDENCY PARSING :\r\n",
        "\r\n",
        "this parsing will first identify the ROOT oword\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxPEvx8H28ZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bb355f-baf5-4001-be65-0cbb74781d3d"
      },
      "source": [
        "from spacy import displacy \r\n",
        "import spacy\r\n",
        "\r\n",
        "nlp =spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "#displacy.render(nlp(text),jupyter=True)\r\n",
        "\r\n",
        "text1 = 'process of language understanding ' \r\n",
        "\r\n",
        "for token in nlp(text1):\r\n",
        "  print(token.text,'=>' , token.dep_, '=>' , token.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process => ROOT => process\n",
            "of => prep => process\n",
            "language => pobj => of\n",
            "understanding => acl => process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__nH38GgmJiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "42367bad-5242-47a1-dd0b-d60a02aea191"
      },
      "source": [
        "import nltk\r\n",
        "!pip install benepar\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.10.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.7.1+cu101)\n",
            "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (4.3.3)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.2.4)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.12.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.1.95)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (53.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[tokenizers,torch]>=4.2.2->benepar) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[tokenizers,torch]>=4.2.2->benepar) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[tokenizers,torch]>=4.2.2->benepar) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[tokenizers,torch]>=4.2.2->benepar) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[tokenizers,torch]>=4.2.2->benepar) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (7.1.2)\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(NP (NP (NN process)) (PP (IN of) (NP (NN language) (NN understanding))) (. .))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhGoPVhfusa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "11629346-d917-408b-cfb0-2be6f7af5788"
      },
      "source": [
        "import benepar\r\n",
        "import nltk\r\n",
        "\r\n",
        "import spacy\r\n",
        "import benepar\r\n",
        "import spacy\r\n",
        "benepar.download('benepar_en3')\r\n",
        "\r\n",
        "from benepar.spacy_plugin import BeneparComponent\r\n",
        "\r\n",
        "nlp = spacy.load('en')\r\n",
        "\r\n",
        "nlp.add_pipe(BeneparComponent('benepar_en3'))\r\n",
        "\r\n",
        "text = 'process of language understanding '\r\n",
        "\r\n",
        "list(nlp(text).sents)[0]._.parse_string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(NP (NP (NN process)) (PP (IN of) (NP (NN language) (NN understanding))))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBp9xLY0e5HF"
      },
      "source": [
        ""
      ]
    }
  ]
}