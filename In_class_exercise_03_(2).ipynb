{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In-class-exercise-03 (2).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaMittapelly/SaiPriyanka_INFO5731_Spring2020/blob/main/In_class_exercise_03_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRl4gOctKYwA"
      },
      "source": [
        "## The third In-class-exercise (9/16/2020, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Loa8sfKYwR"
      },
      "source": [
        "The purpose of this exercise is to under users' information needs, then collect the data for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgLZH1F6KYwT"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "# This is formatted as code\r\n",
        "```\r\n",
        "\r\n",
        "Question 1 (8 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iIIZmiDKYwU"
      },
      "source": [
        "  I want to create a file which has population data of two countries ( INDIA , USA) based on Rank given to them .\n",
        "  The data can be used to compare the population of two cities/sates of two countries based on the rank given to them\n",
        "\n",
        "  step 1 : Serach for the page  where we  get the data about Population categorized on Cities/State and copy /save the URL .\n",
        "\n",
        "          USA :  'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\n",
        "          INDIA : 'https://en.wikipedia.org/wiki/List_of_cities_in_India_by_population'\n",
        "\n",
        "  step 2 : Identify the table or rows where data is present about Population \n",
        "            data is mentioned in the table form for both countries information\n",
        "  step 3 :  As data is mentioned in table format , search for the table name through (INSPECT view ) on web page .\n",
        "            both table is named with same class name\n",
        "            ('table' ,{'class':'wikitable sortable'})\n",
        "\n",
        "  step 4 : after finiding the table , check with the inner tags and try to parse the informations based on the HTML tags\n",
        "            parsing headers , rows \n",
        "\n",
        "            rows = table.find_all(\"tr\")  # parsing the row content \n",
        "\n",
        "  step 5 : start parsing the row content and first we can save the data about column headers\n",
        "            col = [c.text.replace(\"\\n\" ,'') for c in rows[0].find_all('th')]\n",
        "\n",
        "  step 6 : start parsing the information of each row through looping and save in a new variable  \n",
        "            of every iteration \n",
        "\n",
        "            tds2 = rows[i].find_all('td')\n",
        "            values_2  = [td2.text.replace('\\n' , '')\n",
        "                  for td2 in tds2]\n",
        "\n",
        "            \n",
        "\n",
        "  step 7 : create the df and append all the values iterating from the variable \n",
        "\n",
        "                df = df.append(pd.Series(values )  , ignore_index=True)\n",
        "\n",
        "  step 8 : remove the unwanted data or coulumns if necessary \n",
        "\n",
        "            df6 = dfc.drop(columns=['Population(2001)'])\n",
        "\n",
        "  step 9 : concat the dataframes of two countries population data \n",
        "              output = pd.concat([dffinal,df9])\n",
        "\n",
        "  step 10 : import the data from Dataframe  to csv file\n",
        "\n",
        "                output.to_csv('out.csv', index=False)\n",
        "\n",
        "  putting the data into csv file while has population of INDIA & USA (around 600 datasets) as requested in the 2nmd question. \n",
        "\n",
        "Conclusion :\n",
        "Extracting the data from the web page (cities population ) of two different countries , using Beautifulsoup libraries extracted the data\n",
        "with the help of HTML tags and saved them into Dataframe and imported into CSV file "
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOSlgqP5KYwV"
      },
      "source": [
        "Question 2 (12 points): Write python code to collect 500 items of the data you plan to collect above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuPRTD-JKYwW"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup \n",
        "import bs4\n",
        "from urllib.request import urlopen as fopen  \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "my_url = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\n",
        "\n",
        "\n",
        "\n",
        "my_page = fopen(my_url)\n",
        "my_hpage = my_page.read()\n",
        "my_page.close()\n",
        "page = urllib.request.urlopen(my_url)\n",
        "soup = BeautifulSoup(page , 'html.parser')\n",
        "#print(soup.prettify())\n",
        "#table_id = 'wikitable sortable plainrowheaders'\n",
        "table = soup.find('table' ,{'class':'wikitable sortable'}).tbody\n",
        "#print(table)\n",
        "\n",
        "rows = table.find_all(\"tr\")\n",
        "#print(rows)\n",
        "#print(col)\n",
        "col = [c.text.replace(\"\\n\" ,'') for c in rows[0].find_all('th')]\n",
        "df = pd.DataFrame(columns= col)\n",
        "for i in range(1 , len(rows)):\n",
        "\n",
        "  tds = rows[i].find_all('td')\n",
        "  #print(tds)\n",
        "  values  = [td.text.replace('\\n' , '').replace('\\n' , '').replace('\\xa0' , '').replace('\\xa0km2\\n' , 'sq km').replace('\\xa0sq\\xa0mi\\n' , 'sq mi').replace('/sq\\xa0mi\\n' ,'sq mi')  \n",
        "  for td in tds]\n",
        "  #print(values)\n",
        "    \n",
        "\n",
        "  df = df.append(pd.Series(values )  , ignore_index=True)\n",
        "  #df.drop(columns=['2019rank',\t'City',\t'State[c]',\t'2019estimate',\t'2010Census',\t'Change',\t'2016 land area',\t'2016 population density',\t'Location' ])\n",
        "df1 = df.drop(columns=['2019rank',\t'City',\t'State[c]',\t'2019estimate',\t'2010Census',\t'Change',\t'2016 land area',\t'2016 population density',\t'Location'])\n",
        "df1.columns=['Rank ' , ' City_US ' , 'State_US' , '2019 Estimate ' , 'population_US ' ,' change' , ' 2016 land area sq mi ', \n",
        "           '2016 land area sq km  ', '2016 population density sq mi'   , '2016 population density sq km ' , 'location' ]  \n",
        "finaldf = pd.DataFrame(df1)\n",
        "df8 = finaldf.drop(columns=[ '2019 Estimate ',' change' , ' 2016 land area sq mi ', \n",
        "           '2016 land area sq km  ', '2016 population density sq mi'   , '2016 population density sq km ' , 'location'])\n",
        "df9 = pd.DataFrame(df8)\n",
        "df9\n",
        "\n",
        "# second\n",
        "my_url_2 = 'https://en.wikipedia.org/wiki/List_of_cities_in_India_by_population'\n",
        "my_page_2 = fopen(my_url_2)\n",
        "my_hpage = my_page_2.read()\n",
        "my_page_2.close()\n",
        "page_2 = urllib.request.urlopen(my_url_2)\n",
        "soup_2 = BeautifulSoup(page_2 , 'html.parser')\n",
        "#print(soup.prettify())\n",
        "table_2 = soup_2.find('table' ,{'class':'wikitable sortable'}).tbody\n",
        "#print(table)\n",
        "rows_2 = table_2.find_all(\"tr\")\n",
        "#print(rows)\n",
        "col_2 = [c2.text.replace(\"\\n\" ,'') for c2 in rows_2[0].find_all('th')]\n",
        "dfi = pd.DataFrame(columns= col_2)\n",
        "for j in range(1 , len(rows_2)):\n",
        "\n",
        "  tds2 = rows_2[j].find_all('td')\n",
        "  #print(tds)\n",
        "  values_2  = [td2.text.replace('\\n' , '')\n",
        "  for td2 in tds2]\n",
        "  #print(values)\n",
        "  dfi = dfi.append(pd.Series(values_2 ,index=col_2) , ignore_index=True)\n",
        "dfi\n",
        "\n",
        "dfc = dfi.rename(columns={\"Population(2011)[3]\" : \"population_india\",  \"State or union territory\" : \"State_india\" , \"City\"  : \"City_india\"})\n",
        "df6 = dfc.drop(columns=['Population(2001)'])\n",
        "dffinal = pd.DataFrame(df6)\n",
        "\n",
        "\n",
        "output = pd.concat([dffinal,df9])\n",
        "\n",
        "output\n",
        "\n",
        "output.to_csv('out.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 265,
      "outputs": []
    }
  ]
}