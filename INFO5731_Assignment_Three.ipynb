{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaMittapelly/SaiPriyanka_INFO5731_Spring2020/blob/main/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DnpuMLv86JY",
        "outputId": "a14aa4fd-312a-4e73-cfb6-08ba9b968ce9"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df1=pd.read_csv('/content/datacleaning11.csv')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3e2Li6h9m05",
        "outputId": "3742cd12-e9d3-4fc3-effe-34eb2ccc538d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "df = pd.read_csv('/content/datacleaning11.csv')\n",
        "\n",
        "df.head(101)\n",
        "\n",
        "# Convert to list\n",
        "data = df['DATAafter_cleaning'].tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "#print(data)\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "#print(data_words)\n",
        "\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words]])\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "clean_text = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "#print(clean_text)\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(clean_text)\n",
        "\n",
        "# Create Corpus\n",
        "texts = clean_text\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "#print(corpus[:1])\n",
        "\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "print(lda_model.print_topics())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<gensim.interfaces.TransformedCorpus object at 0x7f68ff0e28d0>\n",
            "[(0, '0.045*\"current\" + 0.035*\"focu\" + 0.035*\"chapter\" + 0.024*\"nation\" + 0.024*\"year\" + 0.024*\"work\" + 0.024*\"develop\" + 0.024*\"last\" + 0.024*\"computerassist\" + 0.024*\"thirtyf\"'), (1, '0.046*\"find\" + 0.035*\"word\" + 0.029*\"thesauru\" + 0.018*\"differ\" + 0.018*\"year\" + 0.018*\"gener\" + 0.018*\"keyfact\" + 0.012*\"need\" + 0.012*\"syntact\" + 0.012*\"support\"'), (2, '0.028*\"human\" + 0.028*\"approach\" + 0.014*\"class\" + 0.014*\"automat\" + 0.014*\"way\" + 0.014*\"complex\" + 0.014*\"agent\" + 0.014*\"interpret\" + 0.014*\"result\" + 0.014*\"speak\"'), (3, '0.023*\"effect\" + 0.022*\"test\" + 0.017*\"linguist\" + 0.017*\"state\" + 0.017*\"type\" + 0.012*\"recent\" + 0.012*\"present\" + 0.012*\"also\" + 0.012*\"applic\" + 0.012*\"result\"'), (4, '0.024*\"target\" + 0.024*\"tutor\" + 0.024*\"limit\" + 0.012*\"word\" + 0.012*\"current\" + 0.012*\"automat\" + 0.012*\"object\" + 0.012*\"document\" + 0.012*\"method\" + 0.012*\"gener\"'), (5, '0.033*\"logic\" + 0.026*\"program\" + 0.026*\"word\" + 0.020*\"linguist\" + 0.020*\"process\" + 0.020*\"give\" + 0.020*\"document\" + 0.013*\"model\" + 0.013*\"class\" + 0.013*\"help\"'), (6, '0.040*\"model\" + 0.020*\"recent\" + 0.020*\"particular\" + 0.020*\"develop\" + 0.020*\"present\" + 0.020*\"indian\" + 0.011*\"make\" + 0.011*\"form\" + 0.011*\"work\" + 0.011*\"level\"'), (7, '0.030*\"pattern\" + 0.022*\"work\" + 0.018*\"process\" + 0.018*\"music\" + 0.013*\"statist\" + 0.013*\"extract\" + 0.013*\"confid\" + 0.013*\"limit\" + 0.013*\"develop\" + 0.013*\"understand\"'), (8, '0.025*\"develop\" + 0.017*\"lyric\" + 0.017*\"music\" + 0.017*\"way\" + 0.017*\"concern\" + 0.017*\"statist\" + 0.017*\"includ\" + 0.017*\"review\" + 0.017*\"deep\" + 0.017*\"recent\"'), (9, '0.032*\"visual\" + 0.032*\"automat\" + 0.021*\"develop\" + 0.021*\"model\" + 0.021*\"graph\" + 0.021*\"datum\" + 0.021*\"entri\" + 0.021*\"maximum\" + 0.011*\"understand\" + 0.011*\"perform\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqsPPuTGBEFv",
        "outputId": "4e757b69-76cb-4e34-eec2-69ce03a49cd2"
      },
      "source": [
        "from gensim.models import LdaModel\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "def gensim_lsa_model(dictionary,corpus,number_of_topics,words):\n",
        "    lsamodel = LsiModel(corpus, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    pprint(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n",
        "def coherence_values_lsa(dictionary, corpus, doc_clean, stop, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(corpus, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    print(coherence_values)\n",
        "    return model_list, coherence_values\n",
        "# LSA Model\n",
        "from gensim.models import LsiModel\n",
        "number_of_topics = 10\n",
        "words = 10\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "print(lda_model.print_topics())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.045*\"current\" + 0.035*\"focu\" + 0.035*\"chapter\" + 0.024*\"nation\" + 0.024*\"year\" + 0.024*\"work\" + 0.024*\"develop\" + 0.024*\"last\" + 0.024*\"computerassist\" + 0.024*\"thirtyf\"'), (1, '0.046*\"find\" + 0.035*\"word\" + 0.029*\"thesauru\" + 0.018*\"differ\" + 0.018*\"year\" + 0.018*\"gener\" + 0.018*\"keyfact\" + 0.012*\"need\" + 0.012*\"syntact\" + 0.012*\"support\"'), (2, '0.028*\"human\" + 0.028*\"approach\" + 0.014*\"class\" + 0.014*\"automat\" + 0.014*\"way\" + 0.014*\"complex\" + 0.014*\"agent\" + 0.014*\"interpret\" + 0.014*\"result\" + 0.014*\"speak\"'), (3, '0.023*\"effect\" + 0.022*\"test\" + 0.017*\"linguist\" + 0.017*\"state\" + 0.017*\"type\" + 0.012*\"recent\" + 0.012*\"present\" + 0.012*\"also\" + 0.012*\"applic\" + 0.012*\"result\"'), (4, '0.024*\"target\" + 0.024*\"tutor\" + 0.024*\"limit\" + 0.012*\"word\" + 0.012*\"current\" + 0.012*\"automat\" + 0.012*\"object\" + 0.012*\"document\" + 0.012*\"method\" + 0.012*\"gener\"'), (5, '0.033*\"logic\" + 0.026*\"program\" + 0.026*\"word\" + 0.020*\"linguist\" + 0.020*\"process\" + 0.020*\"give\" + 0.020*\"document\" + 0.013*\"model\" + 0.013*\"class\" + 0.013*\"help\"'), (6, '0.040*\"model\" + 0.020*\"recent\" + 0.020*\"particular\" + 0.020*\"develop\" + 0.020*\"present\" + 0.020*\"indian\" + 0.011*\"make\" + 0.011*\"form\" + 0.011*\"work\" + 0.011*\"level\"'), (7, '0.030*\"pattern\" + 0.022*\"work\" + 0.018*\"process\" + 0.018*\"music\" + 0.013*\"statist\" + 0.013*\"extract\" + 0.013*\"confid\" + 0.013*\"limit\" + 0.013*\"develop\" + 0.013*\"understand\"'), (8, '0.025*\"develop\" + 0.017*\"lyric\" + 0.017*\"music\" + 0.017*\"way\" + 0.017*\"concern\" + 0.017*\"statist\" + 0.017*\"includ\" + 0.017*\"review\" + 0.017*\"deep\" + 0.017*\"recent\"'), (9, '0.032*\"visual\" + 0.032*\"automat\" + 0.021*\"develop\" + 0.021*\"model\" + 0.021*\"graph\" + 0.021*\"datum\" + 0.021*\"entri\" + 0.021*\"maximum\" + 0.011*\"understand\" + 0.011*\"perform\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUQg3TSl823-"
      },
      "source": [
        "Summarize and describe the topic for each cluster.\n",
        "\n",
        "0, '0.045*\"current\" + 0.035*\"focu\" + 0.035*\"chapter\" + 0.024*\"nation\" + 0.024*\"year\" + 0.024*\"work\" + 0.024*\"develop\" + 0.024*\"last\" + 0.024*\"computerassist\" + 0.024*\"thirtyf\"'),\n",
        " (1, '0.046*\"find\" + 0.035*\"word\" + 0.029*\"thesauru\" + 0.018*\"differ\" + 0.018*\"year\" + 0.018*\"gener\" + 0.018*\"keyfact\" + 0.012*\"need\" + 0.012*\"syntact\" + 0.012*\"support\"'),\n",
        " (2, '0.028*\"human\" + 0.028*\"approach\" + 0.014*\"class\" + 0.014*\"automat\" + 0.014*\"way\" + 0.014*\"complex\" + 0.014*\"agent\" + 0.014*\"interpret\" + 0.014*\"result\" + 0.014*\"speak\"'),\n",
        " (3, '0.023*\"effect\" + 0.022*\"test\" + 0.017*\"linguist\" + 0.017*\"state\" + 0.017*\"type\" + 0.012*\"recent\" + 0.012*\"present\" + 0.012*\"also\" + 0.012*\"applic\" + 0.012*\"result\"'),\n",
        " (4, '0.024*\"target\" + 0.024*\"tutor\" + 0.024*\"limit\" + 0.012*\"word\" + 0.012*\"current\" + 0.012*\"automat\" + 0.012*\"object\" + 0.012*\"document\" + 0.012*\"method\" + 0.012*\"gener\"'), \n",
        "(5, '0.033*\"logic\" + 0.026*\"program\" + 0.026*\"word\" + 0.020*\"linguist\" + 0.020*\"process\" + 0.020*\"give\" + 0.020*\"document\" + 0.013*\"model\" + 0.013*\"class\" + 0.013*\"help\"'), \n",
        "(6, '0.040*\"model\" + 0.020*\"recent\" + 0.020*\"particular\" + 0.020*\"develop\" + 0.020*\"present\" + 0.020*\"indian\" + 0.011*\"make\" + 0.011*\"form\" + 0.011*\"work\" + 0.011*\"level\"'), \n",
        "(7, '0.030*\"pattern\" + 0.022*\"work\" + 0.018*\"process\" + 0.018*\"music\" + 0.013*\"statist\" + 0.013*\"extract\" + 0.013*\"confid\" + 0.013*\"limit\" + 0.013*\"develop\" + 0.013*\"understand\"'),\n",
        " (8, '0.025*\"develop\" + 0.017*\"lyric\" + 0.017*\"music\" + 0.017*\"way\" + 0.017*\"concern\" + 0.017*\"statist\" + 0.017*\"includ\" + 0.017*\"review\" + 0.017*\"deep\" + 0.017*\"recent\"'),\n",
        " (9, '0.032*\"visual\" + 0.032*\"automat\" + 0.021*\"develop\" + 0.021*\"model\" + 0.021*\"graph\" + 0.021*\"datum\" + 0.021*\"entri\" + 0.021*\"maximum\" + 0.011*\"understand\" + 0.011*\"perform\"')\n",
        "Summarizing the topic names based on my knowledge with the help of keywords extracted using LDA and LSA model . \n",
        "As that models extracts the keywords that define the sentence by removing unwanted information to get an idea on the topic \n",
        "topic 0 : programming , it has keywords like computerassist , develop , work\n",
        "topic 1 :  support , its may be categorized into supporting something . we can estimate by looking at words find , need , support but not sure \n",
        "topic 3: test results  , its may be categraized into that topic because it has keywords application , results ,test \n",
        "topic 4: goal , its about reaching some target can be identified by words target , limit , object \n",
        "topic 5 : assignmnet , it consistys of keywords that are related to homework , class, help , work , program\n",
        "topic 6:country , it talks about a country(india ) work , development \n",
        "topic 7 : song composition , it consists of keywords work , pattern , develop , music\n",
        "topic 8: Music  , it consists keywords like lyric , music , deep , review\n",
        "topic 9 : development , because its consists of keywords like develop , understand , graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
        "\n",
        "(1) Features (top n-gram phrases) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vATjQNTY8buA",
        "outputId": "31a04e43-a78c-4254-d59a-1b6d8fe059e8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "uCk6LglJZubh",
        "outputId": "626f1461-a09c-4f77-e370-0332fc66c558"
      },
      "source": [
        "#Cleaning the data , however  data was cleaned in previous assignment 3 \n",
        "\n",
        "df1 = pd.read_csv('/content/datacleaning11.csv')\n",
        "df1['Sentimental'] = df1['Sentimental'].replace(['positive'] ,1 )\n",
        "df1['Sentimental'] = df1['Sentimental'].replace(['neutral'] ,1 )\n",
        "df1['Sentimental'] = df1['Sentimental'].replace(['negative'] ,0)\n",
        "df = pd.DataFrame(df1)\n",
        "\n",
        "data1 = df\n",
        "\n",
        "\n",
        "stopset = set(stopwords.words('english'))\n",
        "vectorizer = TfidfVectorizer(use_idf=True , lowercase = True  , stop_words=stopset)\n",
        "\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATAafter_cleaning</th>\n",
              "      <th>Sentimental</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>found</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describ method statist model base maximum entr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seal condit random field term condit term cond...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>address issu cooper linguist gener linguist tr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>descript logic use incom base syntact semant t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>present workbench built priberam informática d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>abstract—natur effect bring improv educ set im...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>twenti year disfavor technolog return imit pro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>statist frequent use stylometri cryptographi s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>summar experi use fragment two rather differ p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   DATAafter_cleaning  Sentimental\n",
              "0                                               found            1\n",
              "1   describ method statist model base maximum entr...            1\n",
              "2   seal condit random field term condit term cond...            1\n",
              "3   address issu cooper linguist gener linguist tr...            1\n",
              "4   descript logic use incom base syntact semant t...            0\n",
              "..                                                ...          ...\n",
              "95  present workbench built priberam informática d...            1\n",
              "96  abstract—natur effect bring improv educ set im...            1\n",
              "97  twenti year disfavor technolog return imit pro...            1\n",
              "98  statist frequent use stylometri cryptographi s...            1\n",
              "99  summar experi use fragment two rather differ p...            1\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSFojWiCx6sO",
        "outputId": "b84cace5-10b3-473a-bbb5-a0006b36ec64"
      },
      "source": [
        "y = df.Sentimental\n",
        "x = vectorizer.fit_transform(df.DATAafter_cleaning)\n",
        "print(y.shape)\n",
        "print(x.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x , y  ,random_state=43)\n",
        "clf = naive_bayes.MultinomialNB()\n",
        "clf.fit(X_train , y_train)\n",
        "\n",
        "\n",
        "accuracy = roc_auc_score(y_test , clf.predict_proba(X_test)[:,-1])\n",
        "\n",
        "print(\"Accuracy {}\" .format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100,)\n",
            "(100, 816)\n",
            "Accuracy 0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL9d8sz9kWbZ",
        "outputId": "9dd669d0-f7ee-4e25-cfd4-5e5a732e8b30"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#print(data1)\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "all_features = vectorizer.fit_transform(data1.DATAafter_cleaning)\n",
        "\n",
        "all_features.shape\n",
        "\n",
        "vectorizer.vocabulary_\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_features , data1.Sentimental , test_size = 0.2 , random_state = 88)\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(X_train , y_train)\n",
        "\n",
        "nr_correct = (y_test == classifier.predict(X_test)).sum()\n",
        "\n",
        "classifier.predict(X_test)\n",
        "\n",
        "Accuracy =  classifier.score(X_test , y_test)\n",
        "print(\" Accuracy {}\" .format(Accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy 0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evtcKvtnx1gU",
        "outputId": "7955c517-9b4d-4614-c14b-90238509243e"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "svm = LinearSVC()\n",
        "xgb = XGBClassifier()\n",
        "scores = cross_val_score(mnb, X_test, y_test, cv=10)\n",
        "print(\"using MNB\",scores.mean())\n",
        "\n",
        "model_svm = svm.fit(X_train,y_train)\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\n",
        "print(classification_report(y_test,y_pred_svm))\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "tfid = TfidfVectorizer()\n",
        "svm = LinearSVC()\n",
        "rf = RandomForestClassifier()\n",
        "scores = cross_val_score(rf, X_test, y_test, cv=10)\n",
        "#print(\"using random forest\",scores.mean())\n",
        "\n",
        "model_xgb = xgb.fit(X_train,y_train)\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_xgb,y_test))\n",
        "print(classification_report(y_test,y_pred_xgb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using MNB 0.9666666666666666\n",
            "Accuracy 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.96      1.00      0.98        24\n",
            "\n",
            "    accuracy                           0.96        25\n",
            "   macro avg       0.48      0.50      0.49        25\n",
            "weighted avg       0.92      0.96      0.94        25\n",
            "\n",
            "Accuracy 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.96      1.00      0.98        24\n",
            "\n",
            "    accuracy                           0.96        25\n",
            "   macro avg       0.48      0.50      0.49        25\n",
            "weighted avg       0.92      0.96      0.94        25\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehRxbswhmR53"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/info5731_spring2021/blob/main/assignment/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}